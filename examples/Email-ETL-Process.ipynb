{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0acf21",
   "metadata": {},
   "source": [
    "# Email Ingestion & RAG Prebuild (Simplified)\n",
    "\n",
    "This notebook now reflects a **concise, production‑lean prebuild** of the email pipeline. We stripped phased prototype noise and focus on clean, single‑responsibility components:\n",
    "\n",
    "Architecture:\n",
    "```\n",
    "┌──────────────────────────┐\n",
    "│ Email Source Connector(s)│  IMAP / (future) Gmail / Exchange\n",
    "└───────────┬──────────────┘\n",
    "            ▼\n",
    "  ┌─────────────────────┐\n",
    "  │ Email Normalizer    │  Parse + canonical record + hashing\n",
    "  └──────────┬──────────┘\n",
    "             ▼\n",
    "┌─────────────────────────┐\n",
    "│ Chunker + Embedder      │  Sentence chunk + vector (stub/real)\n",
    "└───────────┬─────────────┘\n",
    "            ▼\n",
    "  ┌──────────────────────┐      ┌──────────────────────┐\n",
    "  │ SQLite (metadata)    │◀────▶│ Vector Store (Milvus) │\n",
    "  └──────────────────────┘      └──────────────────────┘\n",
    "```\n",
    "\n",
    "Core Principles:\n",
    "- **Separation of concerns**: each class does one thing.\n",
    "- **Stateless helpers** where possible; state isolated to Store / VectorStore.\n",
    "- **Deterministic hashing** for idempotent ingestion.\n",
    "- **Configuration centralized** (imports + config cells only).\n",
    "- **Replaceable implementations**: connectors, embedder, vector store.\n",
    "\n",
    "Cells (after imports/config):\n",
    "1. Connectors & Fixtures\n",
    "2. Normalizer & Hashing\n",
    "3. Persistence Store (SQLite) + VectorStore stub\n",
    "4. Chunker & Embedder\n",
    "5. Orchestrator (run cycle demo)\n",
    "\n",
    "Deprecated former phase cells are minimized or removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee666cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete (imports loaded).\n"
     ]
    }
   ],
   "source": [
    "# Starting Code: Centralized Imports & Global Symbols\n",
    "# Update this cell only when adding/removing libraries. Re-run it, then re-run downstream cells as needed.\n",
    "from __future__ import annotations\n",
    "\n",
    "# Standard Library\n",
    "import hashlib, json, re, textwrap, math, random, sqlite3, time, os, ssl, imaplib, socket, email, quopri, base64\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Any, Optional, Protocol, Iterable, Sequence, Tuple\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import email.utils as eutils\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Protocol, Iterable\n",
    "\n",
    "# NOTE:\n",
    "# - All configurable parameters & runtime tunables are defined in the dedicated Configuration cell (next cell).\n",
    "# - This cell must remain side‑effect light (no network, DB, or socket timeouts). Those are applied in the config cell.\n",
    "# - Add future dependency imports here and re-run this cell before using them elsewhere.\n",
    "\n",
    "print(\"Environment setup complete (imports loaded).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded (file DB mode, names aligned with main app):\n",
      "{'CONTENT_HASH_RECIPE_VERSION': 1, 'DB_MODE': 'databases/knowledgebase.db', 'IMAP_USERNAME': 'tonyphilip@gmail.com', 'EMAIL_CHUNK_SIZE': 600, 'EMBEDDING_MODEL': 'mxbai-embed-large', 'VECTOR_DIM': 1024}\n"
     ]
    }
   ],
   "source": [
    "# Configuration Cell: Centralized tunables & environment parameters\n",
    "# Re-run this cell after any change; downstream cells should only reference these symbols (not hard-code values).\n",
    "\n",
    "# Mailbox / Identity\n",
    "PRIMARY_MAILBOX = os.getenv(\"PRIMARY_MAILBOX\", \"support@example.com\")  # Aligned with app.Config\n",
    "\n",
    "# Networking / IO (lightweight here; heavy network config deferred to app integration)\n",
    "DEFAULT_SOCKET_TIMEOUT = 30  # seconds\n",
    "socket.setdefaulttimeout(DEFAULT_SOCKET_TIMEOUT)\n",
    "\n",
    "# IMAP Connector settings (placeholders for future real connector)\n",
    "IMAP_HOST = os.getenv(\"IMAP_HOST\", \"imap.gmail.com\")\n",
    "IMAP_PORT = int(os.getenv(\"IMAP_PORT\", \"993\"))\n",
    "IMAP_USE_SSL = True\n",
    "IMAP_USERNAME = os.getenv(\"IMAP_USERNAME\", \"user@example.com\")\n",
    "IMAP_PASSWORD = os.getenv(\"IMAP_PASSWORD\", \"CHANGE_ME\")  # NEVER commit real secrets; use env vars in production\n",
    "IMAP_MAILBOX = os.getenv(\"IMAP_MAILBOX\", \"INBOX\")\n",
    "IMAP_BATCH_LIMIT = 50  # soft cap per fetch cycle\n",
    "\n",
    "# Email ingestion alignment with main app Config (placeholders / not all used in notebook yet)\n",
    "EMAIL_ENABLED = False  # feature flag (app.Config.EMAIL_ENABLED)\n",
    "EMAIL_PROVIDER = os.getenv(\"EMAIL_PROVIDER\", \"imap\")\n",
    "EMAIL_SYNC_SINCE_DAYS = int(os.getenv(\"EMAIL_SYNC_SINCE_DAYS\", \"30\"))\n",
    "EMAIL_SYNC_INTERVAL_SECONDS = int(os.getenv(\"EMAIL_SYNC_INTERVAL_SECONDS\", \"300\"))\n",
    "EMAIL_ALLOWED_MIME = os.getenv(\"EMAIL_ALLOWED_MIME\", \"pdf,docx,txt,md,csv\")\n",
    "EMAIL_MAX_ATTACH_SIZE_MB = int(os.getenv(\"EMAIL_MAX_ATTACH_SIZE_MB\", \"8\"))\n",
    "\n",
    "# Dedup / Hashing\n",
    "CONTENT_HASH_RECIPE_VERSION = 1  # increment when compute_content_hash logic changes\n",
    "\n",
    "# Enrichment toggles & limits (heuristics phase)\n",
    "ENABLE_ENRICHMENT = True\n",
    "ENABLE_KEYWORDS = True\n",
    "ENABLE_SUMMARY = True\n",
    "MAX_KEYWORDS = 5\n",
    "\n",
    "# Chunking parameters (aligned with main app naming: EMAIL_CHUNK_SIZE / EMAIL_CHUNK_OVERLAP)\n",
    "# In app these are character-based; here we use them as soft character targets for sentence accumulation.\n",
    "EMAIL_CHUNK_SIZE = int(os.getenv(\"EMAIL_CHUNK_SIZE\", \"600\"))\n",
    "EMAIL_CHUNK_OVERLAP = int(os.getenv(\"EMAIL_CHUNK_OVERLAP\", \"60\"))\n",
    "\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "# Embedding parameters (aligned with app: EMBEDDING_MODEL, VECTOR_DIM)\n",
    "EMBEDDING_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"mxbai-embed-large\")\n",
    "# Main app default VECTOR_DIM=384; we keep a small simulation dim for speed but reuse the same variable name.\n",
    "VECTOR_DIM = int(os.getenv(\"VECTOR_DIM\", \"16\"))  # simulation dimension (app uses 384)\n",
    "\n",
    "DB_DIR = \"databases\"\n",
    "os.makedirs(DB_DIR, exist_ok=True)\n",
    "DB_MODE = os.path.join(DB_DIR, \"knowledgebase.db\")  # file-backed so state persists across runs\n",
    "\n",
    "# Logging / Observability (lightweight placeholders)\n",
    "LOG_LEVEL = \"INFO\"\n",
    "\n",
    "# Stopwords (keyword extraction)\n",
    "STOPWORDS = {\"the\",\"a\",\"an\",\"and\",\"or\",\"of\",\"to\",\"in\",\"on\",\"for\",\"with\",\"at\",\"by\",\"is\",\"it\",\"this\",\"that\"}\n",
    "\n",
    "print(\"Configuration loaded (file DB mode, names aligned with main app):\")\n",
    "print({k: v for k, v in list(globals().items()) if k in [\n",
    "    'IMAP_USERNAME','DB_MODE','CONTENT_HASH_RECIPE_VERSION','EMAIL_CHUNK_SIZE','VECTOR_DIM','EMBEDDING_MODEL'\n",
    "]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa10601",
   "metadata": {},
   "source": [
    "### Email Record Field Reference (Detailed)\n",
    "Each processed email record (pre-DB) uses the following semantics:\n",
    "\n",
    "| Field | Type | Purpose / Semantics | Notes / Future Considerations |\n",
    "|-------|------|--------------------|-------------------------------|\n",
    "| message_id | str | Globally unique RFC822 Message-ID | Uniqueness anchor; collisions rare but validated. |\n",
    "| thread_id | str | Logical conversation/thread root | Simplistic: first reference or self; could upgrade via References chain collapse. |\n",
    "| subject | str | Original Subject line | Normalization (prefix stripping) deferred for display vs. analytics. |\n",
    "| from_addr | str | Sender (lowercased) | Single sender only; malformed multiple ignored. |\n",
    "| to_addrs | list[str] | Primary recipients | Stored as JSON array in DB. |\n",
    "| cc_addrs | list[str] | CC recipients | Optional JSON array. |\n",
    "| date_utc | str (ISO) | Declared sent date/time (UTC) | Not validated vs Received; trust header for now. |\n",
    "| received_utc | str (ISO) | Proxy for first-seen timestamp | Placeholder = date_utc; real pipeline would parse Received headers. |\n",
    "| in_reply_to | str | Single Message-ID replied to | Null if not a reply. |\n",
    "| references_ids | list[str] | Ordered ancestry of Message-IDs | Enables deeper thread reconstruction. |\n",
    "| is_reply | int (0/1) | Heuristic flag for reply | Derived from subject OR In-Reply-To presence. |\n",
    "| is_forward | int (0/1) | Heuristic flag for forward | Subject starts with fwd:/fw: (case-insensitive). |\n",
    "| raw_size_bytes | int | Approx body size pre‑processing | Used for batching / limits. |\n",
    "| body_text | str | Extracted canonical plain text body | HTML stripped / MIME selection to be added later. |\n",
    "| body_html | str or null | Original HTML (if retained) | Currently None; later can store sanitized HTML. |\n",
    "| language | str or null | ISO language code | Deferred: language detection integration. |\n",
    "| has_attachments | int (0/1) | True if any attachments present | Quick filter for enrichment policies. |\n",
    "| attachment_manifest | list[dict] | Lightweight metadata for attachments | JSON (filename,size,mime). No binary content stored. |\n",
    "| processed | int (0/1) | Downstream processing completion marker | Set when embeddings/chunking done (future). |\n",
    "| ingested_at | str (ISO) | DB insertion timestamp | DB default or set programmatically. |\n",
    "| updated_at | str (ISO) | Last mutation timestamp | Null until updates occur. |\n",
    "| content_hash | str | Deterministic dedupe hash | Subject + body + participants_hash + date_utc (current recipe). |\n",
    "| summary | str | Short abstract summary | Stub now; LLM or heuristic later. |\n",
    "| keywords | list[str] | Extracted keywords (naive or LLM) | Stored JSON array. |\n",
    "| auto_topic | str | Heuristic / model inferred topic | May evolve with model versions. |\n",
    "| manual_topic | str | Human override | Never overwritten automatically. |\n",
    "| topic_confidence | float | Confidence score for auto_topic | Range 0..1; stub values now. |\n",
    "| topic_version | int | Version of topic inference algorithm | Increment on model rule change. |\n",
    "| error_state | str | Classification of last processing error | Null if healthy; used for retry logic. |\n",
    "| direction | str | inbound | outbound | cc_only | other | Derived relative to primary mailbox. |\n",
    "| participants | list[str] | Sorted unique participants (from/to/cc) | Facilitates hash, search facets. |\n",
    "| participants_hash | str | Hash of participants list | Stable across ordering differences. |\n",
    "| to_primary | str or null | Echoes PRIMARY_MAILBOX if directly addressed | Aids direct vs peripheral classification. |\n",
    "\n",
    "Guiding Principles:\n",
    "- Keep raw fidelity (subject, addresses) while adding normalized derivatives (hashes, flags).\n",
    "- Avoid premature heavy processing (LLM calls) until base ingestion stable.\n",
    "- All enrichment fields are nullable and additive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be2ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connector Layer (Pluggable)\n",
    "\n",
    "class EmailConnector:\n",
    "    \"\"\"\n",
    "    Abstract connector responsible for retrieving raw email messages from a source\n",
    "    (IMAP, Gmail API, MS Graph, etc.) and returning a list of dict records that\n",
    "    conform to the canonical email schema used downstream.\n",
    "\n",
    "    Implementations SHOULD populate as many fields as are naturally derivable\n",
    "    from the source. Fields not yet computed (e.g., advanced enrichment or\n",
    "    content hashes) MUST still appear with a value of None (or sensible default)\n",
    "    to keep shape stable and simplify later processing stages.\n",
    "\n",
    "    Expected record keys (aligned with the Field Reference table):\n",
    "        message_id          : str\n",
    "        thread_id           : str | None\n",
    "        subject             : str | None\n",
    "        from_addr           : str | None\n",
    "        to_addrs            : list[str]\n",
    "        cc_addrs            : list[str]\n",
    "        date_utc            : str (ISO) | None\n",
    "        received_utc        : str (ISO) | None (placeholder = date_utc)\n",
    "        in_reply_to         : str | None\n",
    "        references_ids      : list[str]\n",
    "        is_reply            : int (0/1)\n",
    "        is_forward          : int (0/1)\n",
    "        raw_size_bytes      : int | None\n",
    "        body_text           : str | None\n",
    "        body_html           : str | None\n",
    "        language            : str | None\n",
    "        has_attachments     : int (0/1)\n",
    "        attachment_manifest : list[dict]\n",
    "        processed           : int (0/1)  (default 0 at ingestion)\n",
    "        ingested_at         : str | None (set later by persistence)\n",
    "        updated_at          : str | None (set later on mutation)\n",
    "        content_hash        : str | None (filled by hashing stage)\n",
    "        summary             : str | None\n",
    "        keywords            : list[str] | None\n",
    "        auto_topic          : str | None\n",
    "        manual_topic        : str | None\n",
    "        topic_confidence    : float | None\n",
    "        topic_version       : int | None\n",
    "        error_state         : str | None\n",
    "        direction           : str | None  (derived later if not set)\n",
    "        participants        : list[str]\n",
    "        participants_hash   : str | None\n",
    "        to_primary          : str | None\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: list of email records (see above)\n",
    "    \"\"\"\n",
    "\n",
    "    def fetch_emails(self, since_date=None) -> list[dict]:\n",
    "        raise NotImplementedError(\"Concrete connectors must implement fetch_emails\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9488cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAP Connection\n",
    "\n",
    "import imaplib, email, quopri, base64\n",
    "from email.header import decode_header, make_header\n",
    "from email.utils import parsedate_to_datetime, getaddresses\n",
    "\n",
    "class IMAPConnector(EmailConnector):\n",
    "    def __init__(self, host, username, password, mailbox=\"INBOX\", batch_limit=IMAP_BATCH_LIMIT):\n",
    "        self.host = host\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.mailbox = mailbox\n",
    "        self.batch_limit = batch_limit\n",
    "\n",
    "    def fetch_emails(self, since_date=None):\n",
    "        conn = imaplib.IMAP4_SSL(self.host) if IMAP_USE_SSL else imaplib.IMAP4(self.host)\n",
    "        conn.login(self.username, self.password)\n",
    "        conn.select(self.mailbox)\n",
    "\n",
    "        criteria = []\n",
    "        if since_date:\n",
    "            criteria.append(f'SINCE \"{since_date.strftime(\"%d-%b-%Y\")}\"')\n",
    "        search_query = \"ALL\" if not criteria else \" \".join(criteria)\n",
    "        status, messages = conn.search(None, search_query)\n",
    "        if status != 'OK':\n",
    "            conn.logout()\n",
    "            return []\n",
    "        email_ids = messages[0].split()\n",
    "        if self.batch_limit:\n",
    "            email_ids = email_ids[-self.batch_limit:]\n",
    "\n",
    "        results = []\n",
    "        for eid in email_ids:\n",
    "            status, msg_data = conn.fetch(eid, \"(RFC822)\")\n",
    "            if status != 'OK' or not msg_data or not msg_data[0]:\n",
    "                continue\n",
    "            try:\n",
    "                msg = email.message_from_bytes(msg_data[0][1])\n",
    "                results.append(self._parse_email(msg))\n",
    "            except Exception as exc:\n",
    "                # Fallback minimal record with error_state populated for visibility\n",
    "                results.append({\n",
    "                    'message_id': None,\n",
    "                    'thread_id': None,\n",
    "                    'subject': None,\n",
    "                    'from_addr': None,\n",
    "                    'to_addrs': [],\n",
    "                    'cc_addrs': [],\n",
    "                    'date_utc': None,\n",
    "                    'received_utc': None,\n",
    "                    'in_reply_to': None,\n",
    "                    'references_ids': [],\n",
    "                    'is_reply': 0,\n",
    "                    'is_forward': 0,\n",
    "                    'raw_size_bytes': None,\n",
    "                    'body_text': None,\n",
    "                    'body_html': None,\n",
    "                    'language': None,\n",
    "                    'has_attachments': 0,\n",
    "                    'attachment_manifest': [],\n",
    "                    'processed': 0,\n",
    "                    'ingested_at': None,\n",
    "                    'updated_at': None,\n",
    "                    'content_hash': None,\n",
    "                    'summary': None,\n",
    "                    'keywords': None,\n",
    "                    'auto_topic': None,\n",
    "                    'manual_topic': None,\n",
    "                    'topic_confidence': None,\n",
    "                    'topic_version': None,\n",
    "                    'error_state': f\"parse_error: {exc.__class__.__name__}\",\n",
    "                    'direction': None,\n",
    "                    'participants': [],\n",
    "                    'participants_hash': None,\n",
    "                    'to_primary': None,\n",
    "                })\n",
    "        conn.logout()\n",
    "        return results\n",
    "\n",
    "    def _decode_header_value(self, raw_val):\n",
    "        if not raw_val:\n",
    "            return None\n",
    "        try:\n",
    "            # make_header handles multiple encoded parts\n",
    "            return str(make_header(decode_header(raw_val))).strip()\n",
    "        except Exception:\n",
    "            parts = decode_header(raw_val)\n",
    "            decoded = []\n",
    "            for text, enc in parts:\n",
    "                if isinstance(text, bytes):\n",
    "                    try:\n",
    "                        decoded.append(text.decode(enc or 'utf-8', errors='ignore'))\n",
    "                    except Exception:\n",
    "                        decoded.append(text.decode('utf-8', errors='ignore'))\n",
    "                else:\n",
    "                    decoded.append(text)\n",
    "            return \"\".join(decoded).strip()\n",
    "\n",
    "    def _parse_email(self, msg):\n",
    "        subject = self._decode_header_value(msg.get('Subject')) or None\n",
    "        message_id = (msg.get('Message-ID') or '').strip() or None\n",
    "        in_reply_to = (msg.get('In-Reply-To') or '').strip() or None\n",
    "        references_raw = msg.get('References') or ''\n",
    "        references_ids = [r.strip('<> ') for r in references_raw.split() if '@' in r] if references_raw else []\n",
    "        date_raw = msg.get('Date')\n",
    "        date_utc = None\n",
    "        if date_raw:\n",
    "            try:\n",
    "                dt = parsedate_to_datetime(date_raw)\n",
    "                if dt and dt.tzinfo is None:\n",
    "                    dt = dt.replace(tzinfo=timezone.utc)\n",
    "                date_utc = dt.astimezone(timezone.utc).isoformat()\n",
    "            except Exception:\n",
    "                date_utc = None\n",
    "        received_utc = date_utc  # placeholder\n",
    "\n",
    "        # Addresses\n",
    "        from_addr = None\n",
    "        raw_from = msg.get('From')\n",
    "        if raw_from:\n",
    "            addrs = getaddresses([raw_from])\n",
    "            if addrs:\n",
    "                from_addr = (addrs[0][1] or addrs[0][0]).lower()\n",
    "        to_addrs = [addr.lower() for _, addr in getaddresses([msg.get('To') or '']) if addr]\n",
    "        cc_addrs = [addr.lower() for _, addr in getaddresses([msg.get('Cc') or '']) if addr]\n",
    "\n",
    "        # Reply / forward heuristics\n",
    "        subj_lc = (subject or '').lower()\n",
    "        is_reply = 1 if (in_reply_to or subj_lc.startswith('re:')) else 0\n",
    "        is_forward = 1 if (subj_lc.startswith('fwd:') or subj_lc.startswith('fw:')) else 0\n",
    "\n",
    "        # Body extraction (prefer text/plain, fallback to first text/html)\n",
    "        body_text, body_html = None, None\n",
    "        attachment_manifest = []\n",
    "        has_attachments = 0\n",
    "        raw_size_bytes = None\n",
    "\n",
    "        if msg.is_multipart():\n",
    "            for part in msg.walk():\n",
    "                ctype = part.get_content_type()\n",
    "                disp = (part.get('Content-Disposition') or '').lower()\n",
    "                if ctype == 'text/plain' and 'attachment' not in disp and body_text is None:\n",
    "                    try:\n",
    "                        body_text = self._decode_part(part)\n",
    "                    except Exception:\n",
    "                        body_text = None\n",
    "                elif ctype == 'text/html' and 'attachment' not in disp and body_html is None:\n",
    "                    try:\n",
    "                        body_html = self._decode_part(part)\n",
    "                    except Exception:\n",
    "                        body_html = None\n",
    "                elif 'attachment' in disp or part.get_filename():\n",
    "                    has_attachments = 1\n",
    "                    fname = self._decode_header_value(part.get_filename()) if part.get_filename() else None\n",
    "                    payload = part.get_payload(decode=True) or b''\n",
    "                    attachment_manifest.append({\n",
    "                        'filename': fname,\n",
    "                        'size': len(payload),\n",
    "                        'mime': ctype,\n",
    "                    })\n",
    "            # size heuristic = sum of part payload sizes\n",
    "            size_acc = 0\n",
    "            for part in msg.walk():\n",
    "                try:\n",
    "                    pl = part.get_payload(decode=True)\n",
    "                    if pl:\n",
    "                        size_acc += len(pl)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            raw_size_bytes = size_acc or None\n",
    "        else:\n",
    "            try:\n",
    "                body_text = self._decode_part(msg)\n",
    "            except Exception:\n",
    "                body_text = None\n",
    "            payload = msg.get_payload(decode=True) or b''\n",
    "            raw_size_bytes = len(payload) if payload else None\n",
    "\n",
    "        participants = sorted({p for p in ([from_addr] if from_addr else []) + to_addrs + cc_addrs})\n",
    "\n",
    "        record = {\n",
    "            'message_id': message_id,\n",
    "            'thread_id': self._derive_thread_id(message_id, in_reply_to, references_ids),\n",
    "            'subject': subject,\n",
    "            'from_addr': from_addr,\n",
    "            'to_addrs': to_addrs,\n",
    "            'cc_addrs': cc_addrs,\n",
    "            'date_utc': date_utc,\n",
    "            'received_utc': received_utc,\n",
    "            'in_reply_to': in_reply_to or None,\n",
    "            'references_ids': references_ids,\n",
    "            'is_reply': is_reply,\n",
    "            'is_forward': is_forward,\n",
    "            'raw_size_bytes': raw_size_bytes,\n",
    "            'body_text': body_text,\n",
    "            'body_html': body_html,\n",
    "            'language': None,\n",
    "            'has_attachments': has_attachments,\n",
    "            'attachment_manifest': attachment_manifest,\n",
    "            'processed': 0,\n",
    "            'ingested_at': None,\n",
    "            'updated_at': None,\n",
    "            'content_hash': None,\n",
    "            'summary': None,\n",
    "            'keywords': None,\n",
    "            'auto_topic': None,\n",
    "            'manual_topic': None,\n",
    "            'topic_confidence': None,\n",
    "            'topic_version': None,\n",
    "            'error_state': None,\n",
    "            'direction': None,  # will be derived later relative to PRIMARY_MAILBOX\n",
    "            'participants': participants,\n",
    "            'participants_hash': None,\n",
    "            'to_primary': PRIMARY_MAILBOX if PRIMARY_MAILBOX.lower() in to_addrs else None,\n",
    "        }\n",
    "        return record\n",
    "\n",
    "    def _decode_part(self, part):\n",
    "        charset = part.get_content_charset() or 'utf-8'\n",
    "        payload = part.get_payload(decode=True)\n",
    "        if not payload:\n",
    "            return None\n",
    "        try:\n",
    "            return payload.decode(charset, errors='ignore')\n",
    "        except Exception:\n",
    "            # Attempt quoted-printable / base64 heuristics\n",
    "            try:\n",
    "                return quopri.decodestring(payload).decode('utf-8', errors='ignore')\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return base64.b64decode(payload).decode('utf-8', errors='ignore')\n",
    "                except Exception:\n",
    "                    return payload.decode('utf-8', errors='ignore')\n",
    "\n",
    "    def _derive_thread_id(self, message_id, in_reply_to, references_ids):\n",
    "        # Simple heuristic: first reference if chain exists, else in_reply_to, else self\n",
    "        if references_ids:\n",
    "            return references_ids[0]\n",
    "        if in_reply_to:\n",
    "            return in_reply_to\n",
    "        return message_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e205508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizer Layer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re, html\n",
    "from typing import Dict, Any\n",
    "\n",
    "class EmailNormalizer:\n",
    "    \"\"\"Email normalization (minimal scope).\n",
    "\n",
    "    Current responsibilities:\n",
    "      - Strip HTML tags (retain plain text)\n",
    "      - Trim quoted reply blocks (heuristic) to keep new content concise\n",
    "      - Remove common signatures / disclaimers / footers\n",
    "      - Normalize whitespace (collapse multiple blank lines / spaces)\n",
    "\n",
    "    Explicitly NOT handled here (deferred to other stages):\n",
    "      - Hash computation (content_hash, participants_hash)\n",
    "      - Direction classification (inbound/outbound/cc_only)\n",
    "      - Summaries or keyword extraction\n",
    "      - Language detection\n",
    "      - Any LLM or model-based enrichment\n",
    "\n",
    "    Design Goal: Keep this idempotent and inexpensive so it can run early\n",
    "    and safely be re-run without altering semantic content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Simple regex patterns for disclaimers / signatures (extend as needed)\n",
    "    DISCLAIMER_PATTERNS = [\n",
    "        r\"^confidentiality notice.*$\",\n",
    "        r\"^this email and any attachments.*$\",\n",
    "        r\"^please consider the environment.*$\",\n",
    "    ]\n",
    "    SIGNATURE_SEPARATORS = [\n",
    "        r\"^--\\s*$\",              # standard signature delimiter\n",
    "        r\"^__+$\",                 # line of underscores\n",
    "        r\"^cheers,?$\",            # informal closings\n",
    "        r\"^regards,?$\",\n",
    "        r\"^best regards,?$\",\n",
    "        r\"^thanks,?$\",\n",
    "        r\"^thank you,?$\",\n",
    "    ]\n",
    "\n",
    "    QUOTED_BLOCK_PATTERNS = [\n",
    "        r\"^>.*$\",                 # classic quoted lines\n",
    "        r\"^on .* wrote:$\",        # common reply intro\n",
    "        r\"^from: .*\",             # forwarded headers\n",
    "        r\"^sent: .*\",\n",
    "        r\"^to: .*\",\n",
    "        r\"^subject: .*\",\n",
    "    ]\n",
    "\n",
    "    def normalize_record(self, record: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        body = record.get('body_text') or record.get('body_html') or ''\n",
    "        cleaned = self.clean_body(body, original_html=record.get('body_html'))\n",
    "        trimmed = self.trim_quoted(cleaned)\n",
    "        stripped = self.strip_disclaimers_and_signatures(trimmed)\n",
    "        collapsed = self.normalize_whitespace(stripped)\n",
    "\n",
    "        record['body_text'] = collapsed or None\n",
    "        # We are not retaining HTML; ensure body_html is None for downstream consistency\n",
    "        record['body_html'] = None\n",
    "        # Leave summary / keywords untouched (remain None)\n",
    "        return record\n",
    "\n",
    "    # --- Cleaning Helpers ---\n",
    "\n",
    "    def clean_body(self, body: str, original_html: str | None = None) -> str:\n",
    "        if not body:\n",
    "            return ''\n",
    "        # Prefer original HTML for fidelity if present\n",
    "        text_source = original_html if (original_html and '<html' in original_html.lower()) else body\n",
    "        soup = BeautifulSoup(text_source, 'html.parser')\n",
    "        # Remove script/style\n",
    "        for tag in soup(['script', 'style']):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text('\\n')\n",
    "        text = html.unescape(text)\n",
    "        return text\n",
    "\n",
    "    def trim_quoted(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return text\n",
    "        lines = text.splitlines()\n",
    "        trimmed = []\n",
    "        for line in lines:\n",
    "            if any(re.match(pat, line.strip().lower()) for pat in self.QUOTED_BLOCK_PATTERNS):\n",
    "                break  # drop everything from first quoted indicator onward\n",
    "            trimmed.append(line)\n",
    "        return '\\n'.join(trimmed) if trimmed else text\n",
    "\n",
    "    def strip_disclaimers_and_signatures(self, text: str) -> str:\n",
    "        if not text:\n",
    "            return text\n",
    "        lines = text.splitlines()\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            lstrip = line.strip().lower()\n",
    "            if any(re.match(pat, lstrip) for pat in self.SIGNATURE_SEPARATORS):\n",
    "                break  # cut remainder after signature separator\n",
    "            if any(re.match(pat, lstrip) for pat in self.DISCLAIMER_PATTERNS):\n",
    "                continue  # skip disclaimer line entirely\n",
    "            cleaned_lines.append(line)\n",
    "        return '\\n'.join(cleaned_lines).strip()\n",
    "\n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        text = re.sub(r\"\\r\", \"\\n\", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6960de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking and Embedding and Storing.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from pymilvus import connections, Collection\n",
    "\n",
    "class EmailProcessor:\n",
    "    def __init__(self, milvus_collection, sqlite_conn):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "        self.embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)\n",
    "        self.milvus_collection = milvus_collection\n",
    "        self.sqlite_conn = sqlite_conn\n",
    "\n",
    "    def process(self, emails):\n",
    "        for email in emails:\n",
    "            chunks = self.text_splitter.split_text(email['body'])\n",
    "            for chunk in chunks:\n",
    "                embedding = self.embedder.embed_query(chunk)\n",
    "                self._store_in_milvus(email['id'], embedding)\n",
    "            self._store_in_sqlite(email)\n",
    "\n",
    "    def _store_in_milvus(self, email_id, embedding):\n",
    "        self.milvus_collection.insert([[email_id], [embedding]])\n",
    "\n",
    "    def _store_in_sqlite(self, email):\n",
    "        cursor = self.sqlite_conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"INSERT OR REPLACE INTO emails (id, subject, sender, recipient, date, body) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (email['id'], email['subject'], email['from'], email['to'], email['date'], email['body'])\n",
    "        )\n",
    "        self.sqlite_conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestration / Normalizer Usage Demo\n",
    "from datetime import datetime, timedelta\n",
    "import hashlib, json\n",
    "\n",
    "def _participants_hash(participants: list[str]) -> str | None:\n",
    "    if not participants:\n",
    "        return None\n",
    "    norm = sorted({(p or '').lower() for p in participants if p})\n",
    "    return hashlib.sha256('\\n'.join(norm).encode('utf-8')).hexdigest()\n",
    "\n",
    "def _content_hash(record: dict) -> str:\n",
    "    payload = {\n",
    "        'v': CONTENT_HASH_RECIPE_VERSION,\n",
    "        'subject': record.get('subject') or '',\n",
    "        'body_text': record.get('body_text') or '',\n",
    "        'date_utc': record.get('date_utc') or '',\n",
    "        'participants_hash': record.get('participants_hash') or '',\n",
    "    }\n",
    "    return hashlib.sha256(json.dumps(payload, sort_keys=True, separators=(',',':')).encode('utf-8')).hexdigest()\n",
    "\n",
    "def _direction(record: dict) -> str:\n",
    "    primary = (PRIMARY_MAILBOX or '').lower()\n",
    "    from_addr = (record.get('from_addr') or '').lower()\n",
    "    to_addrs = [a.lower() for a in (record.get('to_addrs') or [])]\n",
    "    cc_addrs = [a.lower() for a in (record.get('cc_addrs') or [])]\n",
    "    if from_addr == primary:\n",
    "        return 'outbound'\n",
    "    in_to = primary in to_addrs\n",
    "    in_cc = primary in cc_addrs\n",
    "    if in_to and from_addr != primary:\n",
    "        return 'inbound'\n",
    "    if in_cc and not in_to and from_addr != primary:\n",
    "        return 'cc_only'\n",
    "    return 'other'\n",
    "\n",
    "def run_normalization_demo(fetch_limit: int = 3, since_days: int = EMAIL_SYNC_SINCE_DAYS):\n",
    "    normalizer = EmailNormalizer()\n",
    "    if EMAIL_ENABLED:\n",
    "        connector = IMAPConnector(IMAP_HOST, IMAP_USERNAME, IMAP_PASSWORD, IMAP_MAILBOX, batch_limit=fetch_limit)\n",
    "        since_date = datetime.utcnow() - timedelta(days=since_days)\n",
    "        raw_records = connector.fetch_emails(since_date)\n",
    "    else:\n",
    "        # Synthetic sample records (simulate shape pre-normalization)\n",
    "        raw_records = [\n",
    "            {\n",
    "                'message_id': f'<demo-{i}@example.com>',\n",
    "                'thread_id': None,\n",
    "                'subject': f'Demo Email {i}',\n",
    "                'from_addr': 'alice@example.com' if i % 2 == 0 else PRIMARY_MAILBOX,\n",
    "                'to_addrs': [PRIMARY_MAILBOX] if i % 2 == 0 else ['bob@example.com'],\n",
    "                'cc_addrs': ['carol@example.com'] if i == 2 else [],\n",
    "                'date_utc': datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    "                'received_utc': None,\n",
    "                'in_reply_to': None,\n",
    "                'references_ids': [],\n",
    "                'is_reply': 0,\n",
    "                'is_forward': 0,\n",
    "                'raw_size_bytes': None,\n",
    "                'body_text': \"Hello Bob,\\n\\nPlease see below.\\n\\nOn Tue wrote:\\n> older quoted line\",  # contains quoted pattern\n",
    "                'body_html': None,\n",
    "                'language': None,\n",
    "                'has_attachments': 0,\n",
    "                'attachment_manifest': [],\n",
    "                'processed': 0,\n",
    "                'ingested_at': None,\n",
    "                'updated_at': None,\n",
    "                'content_hash': None,\n",
    "                'summary': None,\n",
    "                'keywords': None,\n",
    "                'auto_topic': None,\n",
    "                'manual_topic': None,\n",
    "                'topic_confidence': None,\n",
    "                'topic_version': None,\n",
    "                'error_state': None,\n",
    "                'direction': None,\n",
    "                'participants': [],\n",
    "                'participants_hash': None,\n",
    "                'to_primary': None,\n",
    "            }\n",
    "            for i in range(fetch_limit)\n",
    "        ]\n",
    "        for r in raw_records:\n",
    "            # Fill participants baseline like connector would\n",
    "            participants = sorted({p for p in ([r.get('from_addr')] if r.get('from_addr') else []) + r.get('to_addrs', []) + r.get('cc_addrs', [])})\n",
    "            r['participants'] = participants\n",
    "            r['to_primary'] = PRIMARY_MAILBOX if PRIMARY_MAILBOX.lower() in [a.lower() for a in r.get('to_addrs', [])] else None\n",
    "\n",
    "    print(f\"Raw records: {len(raw_records)}\")\n",
    "\n",
    "    normalized = []\n",
    "    for rec in raw_records:\n",
    "        before_len = len(rec.get('body_text') or '' )\n",
    "        rec = normalizer.normalize_record(rec)\n",
    "        after_len = len(rec.get('body_text') or '' )\n",
    "        rec['participants_hash'] = _participants_hash(rec.get('participants'))\n",
    "        rec['content_hash'] = _content_hash(rec)\n",
    "        rec['direction'] = _direction(rec)\n",
    "        normalized.append(rec)\n",
    "        print(f\"message_id={rec.get('message_id')} body_len {before_len}->{after_len} direction={rec['direction']}\")\n",
    "\n",
    "    # Show sample of final normalized shape (first record)\n",
    "    if normalized:\n",
    "        sample_keys = ['message_id','subject','direction','participants_hash','content_hash','body_text']\n",
    "        preview = {k: (normalized[0].get(k)[:80] + '…' if isinstance(normalized[0].get(k), str) and len(normalized[0].get(k))>83 else normalized[0].get(k)) for k in sample_keys}\n",
    "        print(\"Sample normalized record subset:\")\n",
    "        print(preview)\n",
    "    return normalized\n",
    "\n",
    "# Run demo (will use synthetic data unless EMAIL_ENABLED=True)\n",
    "_ = run_normalization_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
