{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c08949",
   "metadata": {},
   "source": [
    "# ETL Pipeline: Store documents as embeddings in Milvus\n",
    "\n",
    "This notebook is used to test the parts of the document import pipeline for preparing documents for Retrieval-Augmented Generation (RAG) using popular metadata fields for robust traceability, filtering, and retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274d36d",
   "metadata": {},
   "source": [
    "## 1) Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "830123d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_community.vectorstores import Milvus\n",
    "from langchain_core.documents import Document\n",
    "from pymilvus import connections, utility, Collection\n",
    "\n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a84e35",
   "metadata": {},
   "source": [
    "## 2) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4aaee50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "  File: ../staging/Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Embedding model: mxbai-embed-large @ 10.0.10.100\n",
      "  Chat model: llama3:latest @ 10.0.10.100\n",
      "  Unstructured chunking: strategy=basic, max_chars=1000, overlap=200, include_orig=False\n",
      "  Milvus: localhost:19530, collection=documents, drop=False, partition_by_source=True\n",
      "  Document ID (stable): 50c8e45ebb4c603a\n"
     ]
    }
   ],
   "source": [
    "# File configuration\n",
    "# test_file_name = \"RAG2005.11401v4.pdf\"  # Change as needed\n",
    "test_file_name = \"Oxford Collocations Dictionary for Students of English.pdf\"  # Change as needed\n",
    "staging_folder = \"../staging\"\n",
    "test_file_path = os.path.join(staging_folder, test_file_name)\n",
    "\n",
    "# Embeddings (Ollama) configuration\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'mxbai-embed-large')\n",
    "OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "\n",
    "# Chat LLM configuration (for metadata enrichment)\n",
    "CHAT_MODEL = os.getenv('CHAT_MODEL', 'mistral:latest')\n",
    "CHAT_BASE_URL = os.getenv('CHAT_BASE_URL', OLLAMA_HOST)\n",
    "CHAT_TEMPERATURE = float(os.getenv('CHAT_TEMPERATURE', '0.1'))\n",
    "\n",
    "# UnstructuredLoader chunking configuration\n",
    "UNSTRUCTURED_CHUNKING_STRATEGY = os.getenv('UNSTRUCTURED_CHUNKING_STRATEGY', 'basic')\n",
    "UNSTRUCTURED_MAX_CHARACTERS = int(os.getenv('UNSTRUCTURED_MAX_CHARACTERS', '1000'))\n",
    "UNSTRUCTURED_OVERLAP = int(os.getenv('UNSTRUCTURED_OVERLAP', '200'))\n",
    "UNSTRUCTURED_INCLUDE_ORIG = os.getenv('UNSTRUCTURED_INCLUDE_ORIG', 'false').lower() == 'true'\n",
    "\n",
    "# Milvus configuration\n",
    "MILVUS_HOST = os.getenv('MILVUS_HOST', 'localhost')\n",
    "MILVUS_PORT = int(os.getenv('MILVUS_PORT', '19530'))\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'documents')\n",
    "MILVUS_DROP_COLLECTION = os.getenv('MILVUS_DROP_COLLECTION', 'false').lower() == 'true'\n",
    "MILVUS_PARTITION_BY_SOURCE = os.getenv('MILVUS_PARTITION_BY_SOURCE', 'true').lower() == 'true'\n",
    "\n",
    "# Helpers for stable IDs\n",
    "abs_path = str(Path(test_file_path).resolve())\n",
    "file_stat = os.stat(test_file_path) if os.path.exists(test_file_path) else None\n",
    "file_sig = f\"{abs_path}|{file_stat.st_size if file_stat else 0}|{int(file_stat.st_mtime) if file_stat else 0}\"\n",
    "document_id = hashlib.sha1(file_sig.encode('utf-8')).hexdigest()[:16]\n",
    "\n",
    "print(\"Config:\")\n",
    "print(f\"  File: {test_file_path}\")\n",
    "print(f\"  Embedding model: {EMBEDDING_MODEL} @ {OLLAMA_HOST}\")\n",
    "print(f\"  Chat model: {CHAT_MODEL} @ {CHAT_BASE_URL}\")\n",
    "print(f\"  Unstructured chunking: strategy={UNSTRUCTURED_CHUNKING_STRATEGY}, max_chars={UNSTRUCTURED_MAX_CHARACTERS}, overlap={UNSTRUCTURED_OVERLAP}, include_orig={UNSTRUCTURED_INCLUDE_ORIG}\")\n",
    "print(f\"  Milvus: {MILVUS_HOST}:{MILVUS_PORT}, collection={COLLECTION_NAME}, drop={MILVUS_DROP_COLLECTION}, partition_by_source={MILVUS_PARTITION_BY_SOURCE}\")\n",
    "print(f\"  Document ID (stable): {document_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0210f8",
   "metadata": {},
   "source": [
    "## 3) Load Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27097642",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[Document] = []\n",
    "used_unstructured = False\n",
    "loader_chunked = False\n",
    "\n",
    "if not os.path.exists(test_file_path):\n",
    "    raise FileNotFoundError(f'File not found: {test_file_path}')\n",
    "\n",
    "try:\n",
    "    print(\"Using UnstructuredLoader with chunking...\")\n",
    "    loader = UnstructuredLoader(\n",
    "        test_file_path,\n",
    "        chunking_strategy=UNSTRUCTURED_CHUNKING_STRATEGY,\n",
    "        max_characters=UNSTRUCTURED_MAX_CHARACTERS,\n",
    "        overlap=UNSTRUCTURED_OVERLAP,\n",
    "        include_orig_elements=UNSTRUCTURED_INCLUDE_ORIG,\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    used_unstructured = True\n",
    "    loader_chunked = bool(UNSTRUCTURED_CHUNKING_STRATEGY)\n",
    "    print(f\"Loaded {len(documents)} elements via UnstructuredLoader (chunked={loader_chunked})\")\n",
    "except Exception as e:\n",
    "    print(f\"UnstructuredLoader failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Basic stats\n",
    "total_len = sum(len(d.page_content or '') for d in documents)\n",
    "print(f\"Total content length: {total_len:,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43211489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results\n",
    "print(len(documents))\n",
    "for doc in documents:\n",
    "    print(f\"{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1b284",
   "metadata": {},
   "source": [
    "## 4) Setup Metadata and Classification of Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bf315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UnstructuredLoader for OCR and Labeling and Chunking\n",
    "chunks: List[Document] = []\n",
    "\n",
    "print(\"Using chunks from UnstructuredLoader).\")\n",
    "for i, d in enumerate(documents):\n",
    "    text = d.page_content or ''\n",
    "    meta = d.metadata or {}\n",
    "    page = meta.get('page') or meta.get('page_number') or (i + 1)\n",
    "    # Deterministic IDs\n",
    "    content_hash = hashlib.sha1(text.encode('utf-8')).hexdigest()[:16]\n",
    "    meta.update({\n",
    "        'source': test_file_name,\n",
    "        'source_path': abs_path,\n",
    "        'page': page,\n",
    "        'document_id': document_id,\n",
    "        'chunk_id': f\"{document_id}:{content_hash}\",\n",
    "        'content_hash': content_hash,\n",
    "        'content_length': len(text),\n",
    "        'extraction_method': 'unstructured',\n",
    "        'loader_name': 'UnstructuredLoader',\n",
    "        'ocr_used': bool(meta.get('is_ocr') or meta.get('ocr_confidence')),\n",
    "        'processing_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'has_embedding': False,\n",
    "    })\n",
    "    d.metadata = meta\n",
    "    chunks.append(d)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# === LLM-based metadata enrichment (topic classification) ===\n",
    "print(\"\\n🤖 Enriching chunk metadata with LLM classification (topic)...\")\n",
    "chat_llm = ChatOllama(model=CHAT_MODEL, base_url=CHAT_BASE_URL, temperature=CHAT_TEMPERATURE)\n",
    "\n",
    "classification_prompt_tpl = (\n",
    "    \"You are classifying a text chunk for RAG metadata.\\n\"\n",
    "    \"Return ONLY compact JSON with keys: topic.\\n\"\n",
    "    \"topic: concise subject title (3-6 words).\\n\"\n",
    "    \"Text:\\n{chunk}\\n\"\n",
    ")\n",
    "\n",
    "for c in chunks:\n",
    "    snippet = c.page_content[:800]\n",
    "    prompt = classification_prompt_tpl.format(chunk=snippet)\n",
    "    try:\n",
    "        resp = chat_llm.invoke(prompt).content.strip()\n",
    "        start = resp.find('{'); end = resp.rfind('}') + 1\n",
    "        if start != -1 and end > start:\n",
    "            resp_json = json.loads(resp[start:end])\n",
    "            if isinstance(resp_json, dict) and resp_json.get('topic'):\n",
    "                c.metadata['topic'] = resp_json['topic']\n",
    "    except Exception:\n",
    "        c.metadata.setdefault('topic', 'unknown')\n",
    "\n",
    "print(\"LLM enrichment complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview results\n",
    "for doc in documents:\n",
    "    print(f\"{doc.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadddb00",
   "metadata": {},
   "source": [
    "## 5) Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbfe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not chunks:\n",
    "    raise RuntimeError('No chunks to embed')\n",
    "\n",
    "embeddings_model = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)\n",
    "texts = [c.page_content for c in chunks]\n",
    "embs = embeddings_model.embed_documents(texts)\n",
    "print(f\"Embeddings generated: {len(embs)}\")\n",
    "\n",
    "# Validate dimensions and attach embedding metadata\n",
    "dims = {len(vec) for vec in embs if vec is not None}\n",
    "if len(dims) != 1:\n",
    "    raise ValueError(f\"Inconsistent embedding dimensions found: {dims}\")\n",
    "embedding_dim = dims.pop()\n",
    "\n",
    "for i, c in enumerate(chunks):\n",
    "    c.metadata['embedding_model'] = EMBEDDING_MODEL\n",
    "    c.metadata['embedding_dimension'] = embedding_dim\n",
    "    c.metadata['has_embedding'] = True\n",
    "print(f\"Embedding metadata attached to chunks (dim={embedding_dim}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a37bf",
   "metadata": {},
   "source": [
    "## 6) Store in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e165e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Milvus target: {MILVUS_HOST}:{MILVUS_PORT}, collection={COLLECTION_NAME}\")\n",
    "connections.connect('default', host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "collection_name = COLLECTION_NAME\n",
    "metric_type = 'COSINE'\n",
    "index_params = {\"index_type\": \"AUTOINDEX\", \"metric_type\": metric_type}\n",
    "\n",
    "print(f\"Chunks available before dedupe: {len(chunks)}\")\n",
    "\n",
    "# Conditionally drop existing collection based on flag (dev-only)\n",
    "if MILVUS_DROP_COLLECTION and utility.has_collection(collection_name):\n",
    "    print(f\"Dropping existing collection: {collection_name}\")\n",
    "    utility.drop_collection(collection_name)\n",
    "\n",
    "print(f\"Creating and populating collection: {collection_name}\")\n",
    "\n",
    "# Dedupe by content_hash before insert\n",
    "seen = set()\n",
    "unique_docs = []\n",
    "for c in chunks:\n",
    "    ch = (c.metadata or {}).get('content_hash')\n",
    "    if not ch or ch in seen:\n",
    "        continue\n",
    "    seen.add(ch)\n",
    "    unique_docs.append(c)\n",
    "\n",
    "print(f\"Unique chunks after dedupe: {len(unique_docs)}\")\n",
    "if len(unique_docs) != len(chunks):\n",
    "    print(f\"Deduped {len(chunks) - len(unique_docs)} duplicate chunks by content_hash.\")\n",
    "\n",
    "# Prepare texts and metadatas\n",
    "texts = [d.page_content for d in unique_docs]\n",
    "metas = [d.metadata for d in unique_docs]\n",
    "\n",
    "# Sanitize metadata: convert lists/dicts to strings\n",
    "def _sanitize_meta(m):\n",
    "    clean = {}\n",
    "    for k, v in (m or {}).items():\n",
    "        if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "            clean[k] = v\n",
    "        elif isinstance(v, (list, tuple)):\n",
    "            clean[k] = json.dumps(v, ensure_ascii=False)\n",
    "        elif isinstance(v, dict):\n",
    "            clean[k] = json.dumps(v, ensure_ascii=False)\n",
    "        else:\n",
    "            clean[k] = str(v)\n",
    "    return clean\n",
    "\n",
    "metas = [_sanitize_meta(m) for m in metas]\n",
    "\n",
    "# Project metadata to a fixed lean schema and fill defaults\n",
    "def _project_meta(m):\n",
    "    return {\n",
    "        'document_id': str(m.get('document_id', '')),\n",
    "        'source': str(m.get('source', '')),\n",
    "        'page': int(m.get('page', 0) or 0),\n",
    "        'chunk_id': str(m.get('chunk_id', '')),\n",
    "        'topic': str(m.get('topic', '')),\n",
    "        'category': str(m.get('category', '')),\n",
    "        'content_hash': str(m.get('content_hash', '')),\n",
    "        'content_length': int(m.get('content_length', 0) or 0),\n",
    "    }\n",
    "\n",
    "metas = [_project_meta(m) for m in metas]\n",
    "\n",
    "if not texts:\n",
    "    raise RuntimeError(\"No texts to insert into Milvus. Verify earlier steps produced chunks with content.\")\n",
    "\n",
    "# Quick preview\n",
    "print(\"Sample meta:\", metas[0] if metas else {})\n",
    "print(\"Sample text length:\", len(texts[0]) if texts else 0)\n",
    "\n",
    "# Create and populate collection in a single step to ensure schema matches metadatas\n",
    "try:\n",
    "    vectorstore = Milvus.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embeddings_model,\n",
    "        metadatas=metas,\n",
    "        collection_name=collection_name,\n",
    "        connection_args={'host': MILVUS_HOST, 'port': MILVUS_PORT},\n",
    "        index_params=index_params,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Milvus.from_texts failed:\", repr(e))\n",
    "    raise\n",
    "\n",
    "# Ensure collection is flushed, loaded and report stats\n",
    "col = Collection(collection_name)\n",
    "try:\n",
    "    col.flush()\n",
    "except Exception as e:\n",
    "    print(\"Flush warning:\", repr(e))\n",
    "\n",
    "col.load()\n",
    "print(f\"Stored {col.num_entities} chunks in Milvus collection '{collection_name}'.\")\n",
    "\n",
    "# If zero, retry insertion explicitly via add_texts\n",
    "if col.num_entities == 0 and texts:\n",
    "    print(\"Insertion resulted in 0 entities. Retrying with add_texts()...\")\n",
    "    try:\n",
    "        vectorstore.add_texts(texts=texts, metadatas=metas)\n",
    "        try:\n",
    "            col.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "        col.load()\n",
    "        print(f\"After retry, stored {col.num_entities} chunks in Milvus collection '{collection_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(\"Retry with add_texts failed:\", repr(e))\n",
    "\n",
    "# Show index info\n",
    "try:\n",
    "    indexes = col.indexes\n",
    "    print(f\"Indexes: {[i.params for i in indexes]}\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a65f5",
   "metadata": {},
   "source": [
    "## 7) Validate Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a5a1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: retrieval augmented generation\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST http://10.0.10.100:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://10.0.10.100:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST http://10.0.10.100:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "  Content: © VERB + GENERATE help (to) the opportunity to help generate ideas | be used to The wind turbines are used to generate electricity, | be expected to, be likely ...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 356\n",
      "  Chunk ID: 50c8e45ebb4c603a:779ba18696b6fa41\n",
      "  Topic: Idea Generation\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 2:\n",
      "  Content: @ PREP. in a/tha~ I decided to show the results in a bar graph. on ajthe~ We can see on this graph how the com- pany has grown over the last year\n",
      "\n",
      "@ PHRASES in ...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 365\n",
      "  Chunk ID: 50c8e45ebb4c603a:6df2d4625dec23d8\n",
      "  Topic: Computer Graphics\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 3:\n",
      "  Content: In recent years, teachers and students have become increasingly aware of the importance of collocation in English language learning. However, no matter how conv...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 6\n",
      "  Chunk ID: 50c8e45ebb4c603a:e3d2c2a4e81a2f7e\n",
      "  Topic: English Language Learning\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "\n",
      "Query: vector databases\n",
      "------------------------------------------------------------\n",
      "Result 1:\n",
      "  Content: e DATA + NOUN acquisition, capture, collection | entry, Input | storage | access, retrieval | analysis, handling, management, manipulation, processing | exchang...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 205\n",
      "  Chunk ID: 50c8e45ebb4c603a:6f844e6bfe3d9532\n",
      "  Topic: Data Acquisition and Management\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 2:\n",
      "  Content: © VERB + DATA acquire, amass, capture, collect, gather, get, obtain We need ta callect more data before we can do any more wark. | enter, feed in | have, hold, ...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 205\n",
      "  Chunk ID: 50c8e45ebb4c603a:10423c5ed6efb014\n",
      "  Topic: Data Acquisition and Management\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 3:\n",
      "  Content: VERB + INFORMATION contain | have Do you have the information I need? | ratain, store James is able to re- tain an enormous amount of factual information in his...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 428\n",
      "  Chunk ID: 50c8e45ebb4c603a:9fbc0244f4aee8d4\n",
      "  Topic: Information Storage and Retrieval\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "\n",
      "Query: what is a verb in the structure of a sentence\n",
      "------------------------------------------------------------\n",
      "Result 1:\n",
      "  Content: 1 group of words @ ADJ. long, short Try ta keep your sentences short. | complete, whole | broken, incomplete | grammatical, grammatically correct | grammaticall...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 709\n",
      "  Chunk ID: 50c8e45ebb4c603a:664e09c8a38e4088\n",
      "  Topic: Sentence Structure\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 2:\n",
      "  Content: PREP. at althe~ The meeting will be held at a venue in the south of the city. in a/the~ She has performed in venues around Europe. | ~ far The hall is a popular...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 865\n",
      "  Chunk ID: 50c8e45ebb4c603a:e22129d5193a9431\n",
      "  Topic: Grammar and Verb Usage\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n",
      "Result 3:\n",
      "  Content: 2 Find expressions in each column that can be substituted by a single verb. (For example you can do damage to something or just damage something.) Which column ...\n",
      "  Source: Oxford Collocations Dictionary for Students of English.pdf\n",
      "  Page: 923\n",
      "  Chunk ID: 50c8e45ebb4c603a:70645810e933127a\n",
      "  Topic: Grammar and Language Patterns\n",
      "  Category: CompositeElement\n",
      "  Embedding Model: None\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    'retrieval augmented generation',\n",
    "    'vector databases',\n",
    "    'what is a verb in the structure of a sentence'\n",
    "]\n",
    "for q in queries:\n",
    "    print(f\"\\nQuery: {q}\\n{'-'*60}\")\n",
    "    results = vectorstore.similarity_search(q, k=3)\n",
    "    for i, r in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  Content: {r.page_content[:160]}...\")\n",
    "        # Highlight selected metadata fields\n",
    "        meta = r.metadata or {}\n",
    "        print(f\"  Source: {meta.get('source')}\")\n",
    "        print(f\"  Page: {meta.get('page')}\")\n",
    "        print(f\"  Chunk ID: {meta.get('chunk_id')}\")\n",
    "        print(f\"  Topic: {meta.get('topic')}\")\n",
    "        print(f\"  Category: {meta.get('category')}\")\n",
    "        print(f\"  Embedding Model: {meta.get('embedding_model')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325c4cb",
   "metadata": {},
   "source": [
    "## 8) Remove Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e08b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities before deletion: 0\n",
      "Delete expression: document_id == \"50c8e45ebb4c603a\" or source == \"Oxford Collocations Dictionary for Students of English.pdf\"\n",
      "Delete result: (insert count: 0, delete count: 0, upsert count: 0, timestamp: 0, success count: 0, err count: 0\n",
      "Entities after deletion: 0\n",
      "Verification query results: data: [], extra_info: {}\n",
      "✅ Successfully removed all chunks for document: Oxford Collocations Dictionary for Students of English.pdf (ID: 50c8e45ebb4c603a)\n",
      "Entities after deletion: 0\n",
      "Verification query results: data: [], extra_info: {}\n",
      "✅ Successfully removed all chunks for document: Oxford Collocations Dictionary for Students of English.pdf (ID: 50c8e45ebb4c603a)\n"
     ]
    }
   ],
   "source": [
    "# Connect to Milvus (reuse existing connection)\n",
    "connections.connect(\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "# Load the collection\n",
    "collection = Collection(COLLECTION_NAME)\n",
    "collection.load()\n",
    "\n",
    "# Check entities before deletion\n",
    "print(f\"Entities before deletion: {collection.num_entities}\")\n",
    "\n",
    "# Delete records matching either document_id or source\n",
    "# Use expression filter for precise targeting\n",
    "delete_expr = f'document_id == \"{document_id}\" or source == \"{test_file_name}\"'\n",
    "print(f\"Delete expression: {delete_expr}\")\n",
    "\n",
    "# Perform deletion\n",
    "try:\n",
    "    delete_result = collection.delete(expr=delete_expr)\n",
    "    print(f\"Delete result: {delete_result}\")\n",
    "    \n",
    "    # Flush to ensure deletion is persisted\n",
    "    collection.flush()\n",
    "    \n",
    "    # Reload and verify deletion\n",
    "    collection.load()\n",
    "    print(f\"Entities after deletion: {collection.num_entities}\")\n",
    "    \n",
    "    # Verify no records remain for this document\n",
    "    verification_results = collection.query(\n",
    "        expr=delete_expr,\n",
    "        output_fields=[\"document_id\", \"source\", \"chunk_id\"],\n",
    "        limit=10\n",
    "    )\n",
    "    print(f\"Verification query results: {verification_results}\")\n",
    "    \n",
    "    if not verification_results:\n",
    "        print(f\"✅ Successfully removed all chunks for document: {test_file_name} (ID: {document_id})\")\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: {len(verification_results)} chunks still remain\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Deletion failed: {e}\")\n",
    "    # Show what records exist for debugging\n",
    "    try:\n",
    "        existing = collection.query(\n",
    "            expr=f'source == \"{test_file_name}\"',\n",
    "            output_fields=[\"document_id\", \"source\", \"chunk_id\"],\n",
    "            limit=5\n",
    "        )\n",
    "        print(f\"Existing records for {test_file_name}: {existing}\")\n",
    "    except Exception as debug_e:\n",
    "        print(f\"Debug query also failed: {debug_e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099a0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
